{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降 \n",
    "（[Boyd & Vandenberghe, 2004](https://d2l.ai/chapter_references/zreferences.html#boyd-vandenberghe-2004)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch import nn, optim\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('/home/kesci/input')\n",
    "import d2lzh1981 as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一维梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**证明：沿梯度反方向移动自变量可以减小函数值**\n",
    "\n",
    "泰勒展开：\n",
    "\n",
    "$$\n",
    "f(x+\\epsilon)=f(x)+\\epsilon f^{\\prime}(x)+\\mathcal{O}\\left(\\epsilon^{2}\\right)\n",
    "$$\n",
    "\n",
    "代入沿梯度方向的移动量 $\\eta f^{\\prime}(x)$：\n",
    "\n",
    "$$\n",
    "f\\left(x-\\eta f^{\\prime}(x)\\right)=f(x)-\\eta f^{\\prime 2}(x)+\\mathcal{O}\\left(\\eta^{2} f^{\\prime 2}(x)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f\\left(x-\\eta f^{\\prime}(x)\\right) \\lesssim f(x)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "x \\leftarrow x-\\eta f^{\\prime}(x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.\n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2  # Objective function\n",
    "\n",
    "def gradf(x):\n",
    "    return 2 * x  # Its derivative\n",
    "\n",
    "def gd(eta):\n",
    "    x = 10\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * gradf(x)\n",
    "        results.append(x)\n",
    "    print('epoch 10, x:', x)\n",
    "    return results\n",
    "\n",
    "res = gd(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace(res):\n",
    "    n = max(abs(min(res)), abs(max(res)))\n",
    "    f_line = np.arange(-n, n, 0.01)\n",
    "    d2l.set_figsize((3.5, 2.5))\n",
    "    d2l.plt.plot(f_line, [f(x) for x in f_line],'-')\n",
    "    d2l.plt.plot(res, [f(x) for x in res],'-o')\n",
    "    d2l.plt.xlabel('x')\n",
    "    d2l.plt.ylabel('f(x)')\n",
    "    \n",
    "\n",
    "show_trace(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 局部极小值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g.\n",
    "\n",
    "$$\n",
    "f(x) = x\\cos cx\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.15 * np.pi\n",
    "\n",
    "def f(x):\n",
    "    return x * np.cos(c * x)\n",
    "\n",
    "def gradf(x):\n",
    "    return np.cos(c * x) - c * x * np.sin(c * x)\n",
    "\n",
    "show_trace(gd(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多维梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})=\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_{1}}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_{2}}, \\dots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_{d}}\\right]^{\\top}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}+\\epsilon)=f(\\mathbf{x})+\\epsilon^{\\top} \\nabla f(\\mathbf{x})+\\mathcal{O}\\left(\\|\\epsilon\\|^{2}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2d(trainer, steps=20):\n",
    "    x1, x2 = -5, -2\n",
    "    results = [(x1, x2)]\n",
    "    for i in range(steps):\n",
    "        x1, x2 = trainer(x1, x2)\n",
    "        results.append((x1, x2))\n",
    "    print('epoch %d, x1 %f, x2 %f' % (i + 1, x1, x2))\n",
    "    return results\n",
    "\n",
    "def show_trace_2d(f, results): \n",
    "    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
    "    x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))\n",
    "    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
    "    d2l.plt.xlabel('x1')\n",
    "    d2l.plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "f(x) = x_1^2 + 2x_2^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "\n",
    "def f_2d(x1, x2):  # 目标函数\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def gd_2d(x1, x2):\n",
    "    return (x1 - eta * 2 * x1, x2 - eta * 4 * x2)\n",
    "\n",
    "show_trace_2d(f_2d, train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自适应方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 $x + \\epsilon$ 处泰勒展开：\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}+\\epsilon)=f(\\mathbf{x})+\\epsilon^{\\top} \\nabla f(\\mathbf{x})+\\frac{1}{2} \\epsilon^{\\top} \\nabla \\nabla^{\\top} f(\\mathbf{x}) \\epsilon+\\mathcal{O}\\left(\\|\\epsilon\\|^{3}\\right)\n",
    "$$\n",
    "\n",
    "最小值点处满足: $\\nabla f(\\mathbf{x})=0$, 即我们希望 $\\nabla f(\\mathbf{x} + \\epsilon)=0$, 对上式关于 $\\epsilon$ 求导，忽略高阶无穷小，有：\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})+\\boldsymbol{H}_{f} \\boldsymbol{\\epsilon}=0 \\text { and hence } \\epsilon=-\\boldsymbol{H}_{f}^{-1} \\nabla f(\\mathbf{x})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.5\n",
    "\n",
    "def f(x):\n",
    "    return np.cosh(c * x)  # Objective\n",
    "\n",
    "def gradf(x):\n",
    "    return c * np.sinh(c * x)  # Derivative\n",
    "\n",
    "def hessf(x):\n",
    "    return c**2 * np.cosh(c * x)  # Hessian\n",
    "\n",
    "# Hide learning rate for now\n",
    "def newton(eta=1):\n",
    "    x = 10\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * gradf(x) / hessf(x)\n",
    "        results.append(x)\n",
    "    print('epoch 10, x:', x)\n",
    "    return results\n",
    "\n",
    "show_trace(newton())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.15 * np.pi\n",
    "\n",
    "def f(x):\n",
    "    return x * np.cos(c * x)\n",
    "\n",
    "def gradf(x):\n",
    "    return np.cos(c * x) - c * x * np.sin(c * x)\n",
    "\n",
    "def hessf(x):\n",
    "    return - 2 * c * np.sin(c * x) - x * c**2 * np.cos(c * x)\n",
    "\n",
    "show_trace(newton())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(newton(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 收敛性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只考虑在函数为凸函数, 且最小值点上 $f''(x^*) > 0$ 时的收敛速度：\n",
    "\n",
    "令 $x_k$ 为第 $k$ 次迭代后 $x$ 的值， $e_{k}:=x_{k}-x^{*}$ 表示 $x_k$ 到最小值点 $x^{*}$ 的距离，由 $f'(x^{*}) = 0$:\n",
    "\n",
    "$$\n",
    "0=f^{\\prime}\\left(x_{k}-e_{k}\\right)=f^{\\prime}\\left(x_{k}\\right)-e_{k} f^{\\prime \\prime}\\left(x_{k}\\right)+\\frac{1}{2} e_{k}^{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) \\text{for some } \\xi_{k} \\in\\left[x_{k}-e_{k}, x_{k}\\right]\n",
    "$$\n",
    "\n",
    "两边除以 $f''(x_k)$, 有：\n",
    "\n",
    "$$\n",
    "e_{k}-f^{\\prime}\\left(x_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)=\\frac{1}{2} e_{k}^{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)\n",
    "$$\n",
    "\n",
    "代入更新方程 $x_{k+1} = x_{k} - f^{\\prime}\\left(x_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)$, 得到：\n",
    "\n",
    "$$\n",
    "x_k - x^{*} - f^{\\prime}\\left(x_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right) =\\frac{1}{2} e_{k}^{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "x_{k+1} - x^{*} = e_{k+1} = \\frac{1}{2} e_{k}^{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right)\n",
    "$$\n",
    "\n",
    "当 $\\frac{1}{2} f^{\\prime \\prime \\prime}\\left(\\xi_{k}\\right) / f^{\\prime \\prime}\\left(x_{k}\\right) \\leq c$ 时，有:\n",
    "\n",
    "$$\n",
    "e_{k+1} \\leq c e_{k}^{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理 （Heissan阵辅助梯度下降）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\operatorname{diag}\\left(H_{f}\\right)^{-1} \\nabla \\mathbf{x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降与线性搜索（共轭梯度法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降参数更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于有 $n$ 个样本对训练数据集，设 $f_i(x)$ 是第 $i$ 个样本的损失函数, 则目标函数为:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} f_{i}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "其梯度为:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "使用该梯度的一次更新的时间复杂度为 $\\mathcal{O}(n)$\n",
    "\n",
    "随机梯度下降更新公式 $\\mathcal{O}(1)$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f_{i}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "且有：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{i} \\nabla f_{i}(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\mathbf{x})=\\nabla f(\\mathbf{x})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. \n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + 2 x_2^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    return x1 ** 2 + 2 * x2 ** 2  # Objective\n",
    "\n",
    "def gradf(x1, x2):\n",
    "    return (2 * x1, 4 * x2)  # Gradient\n",
    "\n",
    "def sgd(x1, x2):  # Simulate noisy gradient\n",
    "    global lr  # Learning rate scheduler\n",
    "    (g1, g2) = gradf(x1, x2)  # Compute gradient\n",
    "    (g1, g2) = (g1 + np.random.normal(0.1), g2 + np.random.normal(0.1))\n",
    "    eta_t = eta * lr()  # Learning rate at time t\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2)  # Update variables\n",
    "\n",
    "eta = 0.1\n",
    "lr = (lambda: 1)  # Constant learning rate\n",
    "show_trace_2d(f, train_2d(sgd, steps=50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{array}{ll}{\\eta(t)=\\eta_{i} \\text { if } t_{i} \\leq t \\leq t_{i+1}} & {\\text { piecewise constant }} \\\\ {\\eta(t)=\\eta_{0} \\cdot e^{-\\lambda t}} & {\\text { exponential }} \\\\ {\\eta(t)=\\eta_{0} \\cdot(\\beta t+1)^{-\\alpha}} & {\\text { polynomial }}\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential():\n",
    "    global ctr\n",
    "    ctr += 1\n",
    "    return math.exp(-0.1 * ctr)\n",
    "\n",
    "ctr = 1\n",
    "lr = exponential  # Set up learning rate\n",
    "show_trace_2d(f, train_2d(sgd, steps=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial():\n",
    "    global ctr\n",
    "    ctr += 1\n",
    "    return (1 + 0.1 * ctr)**(-0.5)\n",
    "\n",
    "ctr = 1\n",
    "lr = polynomial  # Set up learning rate\n",
    "show_trace_2d(f, train_2d(sgd, steps=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小批量随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "[读取数据](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ch7():  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0) # 标准化\n",
    "    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n",
    "           torch.tensor(data[:1500, -1], dtype=torch.float32) # 前1500个样本(每个样本5个特征)\n",
    "\n",
    "features, labels = get_data_ch7()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t', header=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, states, hyperparams):\n",
    "    for p in params:\n",
    "        p.data -= hyperparams['lr'] * p.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def train_ch7(optimizer_fn, states, hyperparams, features, labels,\n",
    "              batch_size=10, num_epochs=2):\n",
    "    # 初始化模型\n",
    "    net, loss = d2l.linreg, d2l.squared_loss\n",
    "    \n",
    "    w = torch.nn.Parameter(torch.tensor(np.random.normal(0, 0.01, size=(features.shape[1], 1)), dtype=torch.float32),\n",
    "                           requires_grad=True)\n",
    "    b = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "    def eval_loss():\n",
    "        return loss(net(features, w, b), labels).mean().item()\n",
    "\n",
    "    ls = [eval_loss()]\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        start = time.time()\n",
    "        for batch_i, (X, y) in enumerate(data_iter):\n",
    "            l = loss(net(X, w, b), y).mean()  # 使用平均损失\n",
    "            \n",
    "            # 梯度清零\n",
    "            if w.grad is not None:\n",
    "                w.grad.data.zero_()\n",
    "                b.grad.data.zero_()\n",
    "                \n",
    "            l.backward()\n",
    "            optimizer_fn([w, b], states, hyperparams)  # 迭代模型参数\n",
    "            if (batch_i + 1) * batch_size % 100 == 0:\n",
    "                ls.append(eval_loss())  # 每100个样本记录下当前训练误差\n",
    "    # 打印结果和作图\n",
    "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\n",
    "    d2l.set_figsize()\n",
    "    d2l.plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\n",
    "    d2l.plt.xlabel('epoch')\n",
    "    d2l.plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(lr, batch_size, num_epochs=2):\n",
    "    train_ch7(sgd, None, {'lr': lr}, features, labels, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgd(1, 1500, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgd(0.005, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgd(0.05, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数与原书不同的是这里第一个参数优化器函数而不是优化器的名字\n",
    "# 例如: optimizer_fn=torch.optim.SGD, optimizer_hyperparams={\"lr\": 0.05}\n",
    "def train_pytorch_ch7(optimizer_fn, optimizer_hyperparams, features, labels,\n",
    "                    batch_size=10, num_epochs=2):\n",
    "    # 初始化模型\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(features.shape[-1], 1)\n",
    "    )\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)\n",
    "\n",
    "    def eval_loss():\n",
    "        return loss(net(features).view(-1), labels).item() / 2\n",
    "\n",
    "    ls = [eval_loss()]\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        start = time.time()\n",
    "        for batch_i, (X, y) in enumerate(data_iter):\n",
    "            # 除以2是为了和train_ch7保持一致, 因为squared_loss中除了2\n",
    "            l = loss(net(X).view(-1), y) / 2 \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            if (batch_i + 1) * batch_size % 100 == 0:\n",
    "                ls.append(eval_loss())\n",
    "    # 打印结果和作图\n",
    "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\n",
    "    d2l.set_figsize()\n",
    "    d2l.plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\n",
    "    d2l.plt.xlabel('epoch')\n",
    "    d2l.plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pytorch_ch7(optim.SGD, {\"lr\": 0.05}, features, labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
